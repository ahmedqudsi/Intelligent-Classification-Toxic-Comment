# Context-Aware Harmful Language Detection System

# üéØ Objectives

‚û¢Automatically classify user-generated text as harmful or non-harmful <br>
‚û¢Compare multiple machine learning models for text classification <br>
‚û¢Analyze model performance using standard evaluation metrics <br>
‚û¢Provide interpretability using explainable AI methods <br>
‚û¢Build a reproducible and modular NLP pipeline <br>

# üõ†Ô∏è Technology Stack
‚û¢Programming Language: Python<br>
‚û¢Libraries: NumPy, Pandas, Scikit-learn, NLTK, Matplotlib<br>
‚û¢Models: Na√Øve Bayes, Logistic Regression, SVM, Decision Tree, KNN, Random Forest<br>
‚û¢Evaluation Metrics: Accuracy, Precision, Recall, F1-score, AUC‚ÄìROC, Confusion Matrix, Precision‚ÄìRecall Curve<br>
‚û¢Explainability: SHAP, LIME<br>
‚û¢Execution: Windows batch scripts<br> 

# ‚öôÔ∏è System Workflow

‚û¢Input social media comments<br>
‚û¢Text preprocessing (cleaning, tokenization, lemmatization)<br>
‚û¢Feature extraction using vectorization techniques<br>
‚û¢Model training and prediction<br>
‚û¢Performance evaluation and visualization<br>
‚û¢Explainable AI analysis for interpretability<br>

# üìä Results
The experimental analysis shows that ensemble-based models, particularly Random Forest, achieve strong baseline performance across multiple metrics. Transformer-based models demonstrate improved contextual understanding, while explainable AI techniques enhance transparency in decision-making.

# ‚ñ∂Ô∏è How to Run

Install the required dependencies:<br>
pip install -r requirements.txt<br>
Execute the project using:<br>
run.bat<br>
Provide input comments via the CSV file or command line as configured.<br
.docx / *.pdf ‚Äì Research documentation
